{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance Scale Data Set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balance_dataset():\n",
    "    \"\"\"\n",
    "    Load the balance scale dataset from UCI Machine Learning Repository.\n",
    "    Dataset contains information about weight distribution of\n",
    "    balance scales used in physical therapy.\n",
    "    \"\"\"\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data'\n",
    "    column_names = ['class_name', 'left_weight', 'left_distance', 'right_weight', 'right_distance']\n",
    "    dataset = pd.read_csv(url, names=column_names, header=None)\n",
    "      \n",
    "    # Print dataset shape and first few rows\n",
    "    print(\"Dataset shape:\", dataset.shape)\n",
    "    print(\"Dataset head:\\n\", dataset.head())\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Split the dataset into training & test sets using a 70/30 split.\n",
    "    I expect the input dataset to have the class label as the first column\n",
    "    and the feature values as the remaining columns.\n",
    "    \"\"\"\n",
    "    features = dataset.values[:, 1:5]\n",
    "    class_labels = dataset.values[:, 0]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, class_labels, test_size=0.3, random_state=100)\n",
    "\n",
    "    # Print the shapes of the resulting train and test sets\n",
    "    print(f\"Training features shape: {X_train.shape}, labels shape: {y_train.shape}\")\n",
    "    print(f\"Test features shape: {X_test.shape}, labels shape: {y_test.shape}\")\n",
    "\n",
    "    return features, class_labels, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (625, 5)\n",
      "Dataset head:\n",
      "   class_name  left_weight  left_distance  right_weight  right_distance\n",
      "0          B            1              1             1               1\n",
      "1          R            1              1             1               2\n",
      "2          R            1              1             1               3\n",
      "3          R            1              1             1               4\n",
      "4          R            1              1             1               5\n",
      "Training features shape: (437, 4), labels shape: (437,)\n",
      "Test features shape: (188, 4), labels shape: (188,)\n"
     ]
    }
   ],
   "source": [
    "# Load the balance scale dataset\n",
    "dataset = load_balance_dataset()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "features, class_labels, X_train, X_test, y_train, y_test = split_dataset(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging accuracy: 0.7985919098822325\n"
     ]
    }
   ],
   "source": [
    "X = features\n",
    "Y = class_labels\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random_seed = 7\n",
    "\n",
    "# Define a K-fold cross-validation object\n",
    "kfold_cv = KFold(n_splits=10)\n",
    "\n",
    "# Define the base estimator (classifier) for the bagging model\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "# Define the number of trees to use in the bagging model\n",
    "num_trees = 100\n",
    "\n",
    "# Define the bagging model with the base estimator, number of trees, and random seed\n",
    "bagging_model = BaggingClassifier(\n",
    "    base_estimator=base_estimator,\n",
    "    n_estimators=num_trees,\n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# Evaluate the bagging model using cross-validation\n",
    "scores = cross_val_score(\n",
    "    estimator=bagging_model,\n",
    "    X=X,\n",
    "    y=Y,\n",
    "    cv=kfold_cv\n",
    ")\n",
    "\n",
    "# Print the mean accuracy of the bagging model over all cross-validation folds\n",
    "print(f\"Bagging accuracy: {scores.mean()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (fit) the bagging model on the training data\n",
    "trained_bagging_model = bagging_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Calculates and prints the accuracy, confusion matrix, and classification report \n",
    "    for a set of true labels and predicted labels.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array-like): The true labels of the data.\n",
    "    y_pred (array-like): The predicted labels for the data.\n",
    "    \"\"\"\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # Compute the accuracy score\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy: {:.2f}%\".format(acc*100))\n",
    "\n",
    "    # Compute and print the classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 0  7  6]\n",
      " [ 8 77  0]\n",
      " [ 8  3 79]]\n",
      "Accuracy: 82.98%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00        13\n",
      "           L       0.89      0.91      0.90        85\n",
      "           R       0.93      0.88      0.90        90\n",
      "\n",
      "    accuracy                           0.83       188\n",
      "   macro avg       0.60      0.59      0.60       188\n",
      "weighted avg       0.85      0.83      0.84       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the bagging model to make predictions on the test data\n",
    "y_pred_bagging = trained_bagging_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the bagging model predictions\n",
    "calculate_accuracy(y_test, y_pred_bagging)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Accuracy:  0.9118535586277522\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed and number of trees for the AdaBoost model\n",
    "random_seed = 7\n",
    "num_trees = 30\n",
    "\n",
    "# Set up the k-fold cross validation for the model\n",
    "kfold_cv = KFold(n_splits=10)\n",
    "\n",
    "# Initialize the AdaBoost classifier with the given parameters\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=random_seed)\n",
    "\n",
    "# Train the model and evaluate its accuracy using k-fold cross validation\n",
    "scores = cross_val_score(model, X, Y, cv=kfold_cv)\n",
    "print(\"Adaboost Accuracy: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an AdaBoost model using the training data\n",
    "trained_adaboost_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 3  5  5]\n",
      " [ 2 83  0]\n",
      " [ 6  0 84]]\n",
      "Accuracy: 90.43%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           B       0.27      0.23      0.25        13\n",
      "           L       0.94      0.98      0.96        85\n",
      "           R       0.94      0.93      0.94        90\n",
      "\n",
      "    accuracy                           0.90       188\n",
      "   macro avg       0.72      0.71      0.72       188\n",
      "weighted avg       0.90      0.90      0.90       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the trained AdaBoost model to make predictions on the test data\n",
    "y_pred_adaboost = trained_adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the AdaBoost model predictions\n",
    "calculate_accuracy(y_test, y_pred_adaboost)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy :  0.8050179211469534\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "random_seed = 7\n",
    "\n",
    "# Define the number of trees to use in the random forest\n",
    "num_trees = 100\n",
    "\n",
    "# Define the maximum number of features to consider when splitting each tree\n",
    "max_features = 3\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "kfold_cv = KFold(n_splits=10)\n",
    "\n",
    "# Create a random forest classifier with the specified number of trees and maximum number of features\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features, random_state=random_seed)\n",
    "\n",
    "# Evaluate the random forest classifier using cross-validation\n",
    "scores = cross_val_score(model, X, Y, cv=kfold_cv)\n",
    "print(\"Random Forest Accuracy : \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest model on the training data\n",
    "trained_random_forest_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 0  7  6]\n",
      " [ 8 77  0]\n",
      " [ 7  5 78]]\n",
      "Accuracy: 82.45%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00        13\n",
      "           L       0.87      0.91      0.89        85\n",
      "           R       0.93      0.87      0.90        90\n",
      "\n",
      "    accuracy                           0.82       188\n",
      "   macro avg       0.60      0.59      0.59       188\n",
      "weighted avg       0.84      0.82      0.83       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the trained random forest model to make predictions on the test data\n",
    "y_pred_random_forest = trained_random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the random forest model predictions\n",
    "calculate_accuracy(y_test, y_pred_random_forest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a decision tree classifier using the Gini Index\n",
    "def train_decision_tree_gini(X_train, X_test, y_train): \n",
    "    # Create the decision tree classifier object with Gini Index as criterion\n",
    "    clf_gini = DecisionTreeClassifier(criterion=\"gini\", \n",
    "                                       random_state=100,\n",
    "                                       max_depth=3,\n",
    "                                       min_samples_leaf=5) \n",
    "  \n",
    "    # Train the classifier on the training data\n",
    "    clf_gini.fit(X_train, y_train) \n",
    "    return clf_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train decision tree using entropy criterion\n",
    "def train_using_entropy(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Trains a decision tree classifier using entropy as the split criterion\n",
    "    \n",
    "    Args:\n",
    "    - X_train (array-like, shape = [n_samples, n_features]): Training input samples\n",
    "    - X_test (array-like, shape = [n_samples, n_features]): Test input samples\n",
    "    - y_train (array-like, shape = [n_samples]): Target values for the training set\n",
    "    \n",
    "    Returns:\n",
    "    - clf_entropy (DecisionTreeClassifier): Fitted decision tree classifier using entropy as the split criterion\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create decision tree classifier object\n",
    "    clf_entropy = DecisionTreeClassifier( \n",
    "            criterion = \"entropy\", \n",
    "            random_state = 100, \n",
    "            max_depth = 3, \n",
    "            min_samples_leaf = 5) \n",
    "  \n",
    "    # Fit decision tree classifier to training data\n",
    "    clf_entropy.fit(X_train, y_train) \n",
    "    \n",
    "    # Return the trained classifier\n",
    "    return clf_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions using a trained classifier\n",
    "def make_predictions(X_test, clf): \n",
    "\n",
    "    # Predictions using the classifier \n",
    "    y_pred = clf.predict(X_test) \n",
    "    # Print the predicted values (optional)\n",
    "    # print(\"Predicted values:\") \n",
    "    # print(y_pred) \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Phase \n",
    "clf_gini = train_decision_tree_gini(X_train, X_test, y_train) \n",
    "clf_entropy = train_using_entropy(X_train, X_test, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 0  6  7]\n",
      " [ 0 67 18]\n",
      " [ 0 19 71]]\n",
      "Accuracy: 73.40%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00        13\n",
      "           L       0.73      0.79      0.76        85\n",
      "           R       0.74      0.79      0.76        90\n",
      "\n",
      "    accuracy                           0.73       188\n",
      "   macro avg       0.49      0.53      0.51       188\n",
      "weighted avg       0.68      0.73      0.71       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Make predictions using the gini model\n",
    "y_pred_gini = make_predictions(X_test, clf_gini)\n",
    "\n",
    "# Calculate accuracy of the gini model\n",
    "calculate_accuracy(y_test, y_pred_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 0  6  7]\n",
      " [ 0 63 22]\n",
      " [ 0 20 70]]\n",
      "Accuracy: 70.74%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00        13\n",
      "           L       0.71      0.74      0.72        85\n",
      "           R       0.71      0.78      0.74        90\n",
      "\n",
      "    accuracy                           0.71       188\n",
      "   macro avg       0.47      0.51      0.49       188\n",
      "weighted avg       0.66      0.71      0.68       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict using the entropy classifier\n",
    "y_pred_entropy = make_predictions(X_test, clf_entropy) \n",
    "\n",
    "# Calculate and print the accuracy of the entropy classifier\n",
    "calculate_accuracy(y_test, y_pred_entropy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect on the applicability of the methods studied"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief summary of my Jupyter notebook, based on cross-validation accuracies:\n",
    "\n",
    "    Bagging: 79.86%\n",
    "    AdaBoost: 91.19%\n",
    "    Random Forest: 80.50%\n",
    "    Decision Tree (Gini): 73.40%\n",
    "    Decision Tree (Entropy): 70.74%\n",
    "\n",
    "AdaBoost has the highest cross-validation accuracy (91.19%), followed by the Random Forest and Bagging. The decision tree classifiers have lower accuracies, as expected. AdaBoost, Bagging, and Random Forest provide better performance as they combine the results of multiple weak decision trees to improve the overall prediction accuracy and reduce overfitting. When comparing the performance on the test set, AdaBoost again has the highest accuracy (90.43%). The confusion matrices and classification reports provide further insight into the performance of each classifier.\n",
    "\n",
    "While AdaBoost has the best performance, it's important to consider that the choice of performance metrics depends on the specific requirements. For example, if the cost of misclassifying a certain class is higher than others, it would be preferable to focus on precision, recall or F1-score for that class rather than overall accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
